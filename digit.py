import os
import sys
import json
import time
import hashlib
import hmac
import base64
import sqlite3
import logging
import threading
import multiprocessing
import queue
import socket
import ssl
from datetime import datetime, timedelta
from pathlib import Path
from typing import (
    Any,
    Dict,
    List,
    Tuple,
    Optional,
    Callable,
    Generator,
    Union,
    TypeVar
)
from enum import Enum, auto
from dataclasses import dataclass, field
from functools import wraps, lru_cache
from urllib.parse import urlparse
from collections import deque, defaultdict, OrderedDict
from accelerate import infer_auto_device_map
import numpy as np
import pandas as pd
import requests
import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoConfig,
    TextIteratorStreamer,
    BitsAndBytesConfig,
    pipeline,
    StoppingCriteria,
    StoppingCriteriaList
)
from pydantic import BaseModel, Field, validator
from cryptography.hazmat.primitives import hashes, hmac
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend
from cryptography.fernet import Fernet, MultiFernet
from retrying import retry
from ratelimiter import RateLimiter
from prometheus_client import (
    start_http_server,
    Counter,
    Gauge,
    Histogram,
    Summary
)
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from functools import lru_cache
from requests.exceptions import RequestException
# ------------------ Initialization ------------------
load_dotenv("configs/.env")
config = json.load(open("configs/system_config.json"))
VERSION = "7.9.2"
MAX_HISTORY = 5000
CRYPTO_ITERATIONS = 1000000
MODEL_ENSEMBLE_SIZE = 5
FALLBACK_THRESHOLD = 0.85
REALTIME_UPDATE_INTERVAL = 300  # seconds

# ------------------ Type Aliases ------------------
TensorBatch = TypeVar('TensorBatch', bound=torch.Tensor)
ModelOutput = Union[str, Dict[str, Any], List[Any]]

# ------------------ Enums ------------------
class IntentType(Enum):
    CODE = auto()
    SEARCH = auto()
    ANALYTICS = auto()
    CONVERSATIONAL = auto()
    SYSTEM = auto()

class SecurityLevel(Enum):
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4
#
@dataclass(frozen=True)
class NeuralConfig:
    model_name: str
    quantization: BitsAndBytesConfig
    lora_config: Optional[Dict] = None
    adapter_path: Optional[Path] = None
    trust_remote_code: bool = True
    device_map: str = "auto"

@dataclass
class CognitiveState:
    short_term_memory: deque = field(default_factory=lambda: deque(maxlen=500))
    long_term_memory: OrderedDict = field(default_factory=OrderedDict)
    emotional_context: Dict[str, float] = field(default_factory=lambda: {
        'joy': 0.5,
        'anger': 0.0,
        'curiosity': 0.7
    })
    attention_weights: Dict[str, float] = field(default_factory=lambda: {
        'technical': 0.6,
        'creative': 0.4
    })

# ------------------ Prometheus Metrics ------------------
REQUEST_COUNTER = Counter('total_requests', 'Total API requests')
RESPONSE_TIME = Histogram('response_time', 'Response time distribution')
ERROR_COUNTER = Counter('errors', 'Total errors', ['type'])
MODEL_LOAD_GAUGE = Gauge('models_loaded', 'Currently loaded models')

class QuantumSecurity:
    def __init__(self):
        self.key = PBKDF2HMAC(
            algorithm=hashes.SHA3_512(),
            length=64,
            salt=os.urandom(16),
            iterations=CRYPTO_ITERATIONS
        ).derive(os.getenv("QUANTUM_SECRET").encode())
        self.hmac_key = os.urandom(64)

    def encrypt(self, data: str) -> bytes:
        nonce = os.urandom(16)
        cipher = AES.new(self.key, AES.MODE_GCM, nonce=nonce)
        ct, tag = cipher.encrypt_and_digest(data.encode())
        hmac = HMAC(self.hmac_key, hashes.SHA3_512())
        hmac.update(nonce + ct + tag)
        return base64.b64encode(nonce + tag + ct + hmac.finalize())

    def decrypt(self, enc_data: bytes) -> str:
        data = base64.b64decode(enc_data)
        nonce, tag, ct, mac = data[:16], data[16:32], data[32:-64], data[-64:]
        hmac = HMAC(self.hmac_key, hashes.SHA3_512())
        hmac.update(nonce + ct + tag)
        hmac.verify(mac)
        cipher = AES.new(self.key, AES.MODE_GCM, nonce=nonce)
        return cipher.decrypt_and_verify(ct, tag).decode()

class DistributedKeyManager:
    def __init__(self, nodes: List[str]):
        self.nodes = nodes
        self.quorum = len(nodes) // 2 + 1
        self.key_fragments = defaultdict(dict)
        self.lock = threading.Lock()
        self.session_key = None
    def _generate_key_shares(self, secret: bytes) -> Dict[str, bytes]:
        from secrets import token_bytes
        from hashlib import sha3_512
        key = sha3_512(secret).digest()
        shares = {}
        for i, node in enumerate(self.nodes):
            share = token_bytes(64)
            shares[node] = sha3_512(key + share).digest()
        return shares
    def distribute_key(self, secret: bytes) -> bool:
        shares = self._generate_key_shares(secret)
        with self.lock:
            for node, share in shares.items():
                try:
                    response = requests.post(
                        f"https://{node}/key",
                        data=share,
                        timeout=2
                    )
                    if response.status_code != 200:
                        return False
                except Exception:
                    return False
            return True
    def reconstruct_key(self) -> Optional[bytes]:
        collected = []
        for node in self.nodes:
            try:
                response = requests.get(f"https://{node}/key", timeout=1)
                if response.status_code == 200:
                    collected.append(response.content)
            except Exception:
                continue
            if len(collected) >= self.quorum:
                from functools import reduce
                from operator import xor
                return reduce(xor, collected)
        return None
class TriadCognitiveSystem:
    def __init__(self):
        self.security = QuantumSecurity()
        self.models = self._load_models()
        self.spellcheck = pipeline("text2text-generation", model=config["apis"]["huggingface"]["models"]["spellcheck"])
        # ØªØºÛŒÛŒØ± Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Gamma
        self.validator = AutoModelForSequenceClassification.from_pretrained(
            config["models"]["gamma"]["name"],
            device_map="auto",
            torch_dtype=torch.float16
        )
        self.validator_tokenizer = AutoTokenizer.from_pretrained(
            config["models"]["gamma"]["name"]
        )
        self.api = APIOracle()
        self.memory = MemoryVault()
        self.executor = ThreadPoolExecutor(max_workers=5)
        
    def _load_models(self) -> Dict:
        models = {}
        for model_id in ["alpha", "beta", "gamma"]:
            cfg = config["models"][model_id]
            models[model_id] = {
                "model": AutoModelForCausalLM.from_pretrained(
                    cfg["name"],
                    quantization_config=BitsAndBytesConfig(**cfg.get("quantization", {})),
                    device_map="auto"
                ),
                "tokenizer": AutoTokenizer.from_pretrained(cfg["name"])
            }
        return models
    
    @lru_cache(maxsize=5000)
    def _correct_spelling(self, text: str) -> str:
        return self.spellcheck(text, max_length=len(text)*2)[0]['generated_text']
    
    def _validate_fact(self, claim: str) -> float:
        inputs = self.models["gamma"]["tokenizer"](claim, return_tensors="pt")
        outputs = self.validator(**inputs)
        return torch.sigmoid(outputs.logits).item()
    
    def _ensemble_response(self, responses: List[str]) -> str:
        weights = np.array([config["models"][m]["weight"] for m in ["alpha", "beta", "gamma"]])
        embeddings = np.array([self._get_embedding(r) for r in responses])
        weighted_avg = np.dot(weights, embeddings)
        return self._nearest_neighbor(weighted_avg, responses)
    
    def _get_embedding(self, text: str) -> np.ndarray:
        inputs = self.models["alpha"]["tokenizer"](text, return_tensors="pt")
        outputs = self.models["alpha"]["model"].base_model(**inputs, output_hidden_states=True)
        return outputs.hidden_states[-1].mean(dim=1).cpu().detach().numpy()
    
    def _nearest_neighbor(self, vector: np.ndarray, candidates: List[str]) -> str:
        embeddings = np.array([self._get_embedding(c) for c in candidates])
        distances = np.linalg.norm(embeddings - vector, axis=1)
        return candidates[np.argmin(distances)]
    
    def _process_input(self, user_input: str) -> Dict:
        corrected = self._correct_spelling(user_input)
        intent = self.api.detect_intent(corrected)
        sentiment = self.api.analyze_sentiment(corrected)
        return {
            "original": user_input,
            "corrected": corrected,
            "intent": intent,
            "sentiment": sentiment,
            "hash": self.security.generate_hash(corrected)
        }
    
    def _generate_response(self, processed_input: Dict) -> str:
        model_responses = []
        
        # Generate responses from all models
        for model_id in ["alpha", "beta", "gamma"]:
            model = self.models[model_id]["model"]
            tokenizer = self.models[model_id]["tokenizer"]
            
            inputs = tokenizer(
                f"INTENT: {processed_input['intent']}\nQUERY: {processed_input['corrected']}",
                return_tensors="pt",
                max_length=8192
            ).to(model.device)
            
            outputs = model.generate(**inputs, max_new_tokens=4096)
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            model_responses.append(response.split("RESPONSE:")[-1].strip())
        
        # Ensemble and validate final response
        final_response = self._ensemble_response(model_responses)
        if self._validate_fact(final_response) < config["system"]["accuracy_threshold"]:
            final_response = "Ù…Ù†Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù…. Ù„Ø·ÙØ§ Ø³ÙˆØ§Ù„ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± ÙØ±Ù…Ø§ÛŒÛŒØ¯."
        
        return final_response
def _validate_fact(self, claim: str) -> float:
        inputs = self.validator_tokenizer(
            claim,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(self.validator.device)
        with torch.inference_mode():
            outputs = self.validator(**inputs)
        return torch.sigmoid(outputs.logits).item()

    def process_query(self, user_input: str) -> Dict:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø§Ø±Ø¨Ø± Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ"""
        try:
            processed = self._process_input(user_input)
            response = self._generate_response(processed)
            
            # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø§Ø³Ø®
            if not self._final_validation(response):
                response = self._fallback_strategy(user_input)
            
            return self._package_response(processed, response)
        except Exception as e:
            logging.error(f"Critical Error: {str(e)}")
            return self._emergency_response()

    def _final_validation(self, response: str) -> bool:
        # Ø¨Ø±Ø±Ø³ÛŒ Ú†Ù†Ø¯Ù„Ø§ÛŒÙ‡ Ø§Ù…Ù†ÛŒØªÛŒ Ùˆ Ù…Ø¹Ù†Ø§ÛŒÛŒ
        checks = [
            self._security_check(response),
            self._semantic_coherence_check(response),
            self._fact_validation_check(response)
        ]
        return all(checks)
class HybridModelEnsemble:
    def __init__(self):
        self.models = []
        self.tokenizers = []
        self._init_ensemble()
        self.adaptive_tokenizer = RealtimeAdaptiveTokenizer(
            AutoTokenizer.from_pretrained("parsi/tokenizer-base")
        )

    def _init_ensemble(self):
        for model_cfg in config["models"].values():
            model = AutoModelForCausalLM.from_pretrained(
                model_cfg["name"],
                device_map="auto",
                quantization_config=BitsAndBytesConfig(**model_cfg["quantization"])
            )
            tokenizer = AutoTokenizer.from_pretrained(model_cfg["name"])
            self.models.append(model)
            self.tokenizers.append(tokenizer)
class APIOracle:
    def __init__(self):
        self.headers = {
            "GitHub": {"Authorization": f"Bearer {os.getenv('GITHUB_PAT')}"},
            "HuggingFace": {"Authorization": f"Bearer {os.getenv('HF_TOKEN')}"}
        }
    
    def detect_intent(self, text: str) -> str:
        response = requests.post(
            f"{config['apis']['huggingface']['inference']}/intent-detection",
            json={"inputs": text},
            headers=self.headers["HuggingFace"]
        )
        return response.json()[0]['label']
    
    def analyze_sentiment(self, text: str) -> Dict:
        response = requests.post(
            f"{config['apis']['huggingface']['inference']}/sentiment-analysis",
            json={"inputs": text},
            headers=self.headers["HuggingFace"]
        )
        return response.json()[0]
    
    def google_verify(self, claim: str) -> List[Dict]:
        params = {
            "key": os.getenv("GOOGLE_API_KEY"),
            "cx": os.getenv("GOOGLE_CSE_ID"),
            "q": f"verify {claim}",
            "num": 3
        }
        return requests.get(config["apis"]["google"]["search"], params=params).json().get("items", [])
# ------------------ DistributedKeyManager ------------------
class GoogleServiceHandler:
    def __init__(self):
        self.base_url = config["apis"]["google"]["search_endpoint"]
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.cx = os.getenv("GOOGLE_CSE_ID")

    @CircuitBreaker()
    def call(self, endpoint: str, params: dict):
        full_url = f"{self.base_url}/{endpoint}"
        params.update({
            "key": self.api_key,
            "cx": self.cx
        })
        return requests.get(full_url, params=params, timeout=3)

class GitHubServiceHandler:
    def __init__(self):
        self.base_url = config["apis"]["github"]["graphql_endpoint"]
        self.headers = {
            "Authorization": f"Bearer {os.getenv('GITHUB_PAT')}",
            "Content-Type": "application/json"
        }

    @CircuitBreaker()
    def call(self, endpoint: str, params: dict):
        query = """
        query ($repo: String!, $owner: String!) {
            repository(name: $repo, owner: $owner) {
                name
                description
                stargazers { totalCount }
                issues(states: OPEN) { totalCount }
            }
        }
        """
        return requests.post(
            self.base_url,
            json={"query": query, "variables": params},
            headers=self.headers,
            timeout=5
        )

# ------------------ RealtimeAdaptiveTokenizer ------------------
class RealtimeAdaptiveTokenizer:
    def __init__(self, base_tokenizer):
        self.base_tokenizer = base_tokenizer
        self.pattern_cache = LRUCache(1000)
        self.dynamic_rules = []
        self.frequency = defaultdict(int)
        self.lock = threading.Lock()

    def _detect_patterns(self, text: str) -> List[str]:
        from collections import deque
        patterns = []
        window = deque(maxlen=5)
        for token in self.base_tokenizer.tokenize(text):
            window.append(token)
            if len(window) == 5:
                patterns.append("_".join(window))
                self.frequency[patterns[-1]] += 1
        return patterns

    def adapt(self, text: str) -> List[str]:
        patterns = self._detect_patterns(text)
        with self.lock:
            for pattern in patterns:
                if self.frequency[pattern] > 10 and pattern not in self.dynamic_rules:
                    self.dynamic_rules.append(pattern)
                    self.base_tokenizer.add_tokens([pattern])
        return self.base_tokenizer.tokenize(text)

# ------------------ EmotionalIntelligenceModule ------------------
class EmotionalIntelligenceModule:
    EMOTION_LEXICON = {
        'Ø®ÙˆØ´Ø­Ø§Ù„': ['Ø¹Ø§Ù„ÛŒ', 'Ù…Ù…Ù†ÙˆÙ†', 'Ø¹Ø§Ù„ÛŒÙ‡'],
        'Ø¹ØµØ¨Ø§Ù†ÛŒ': ['Ù†Ø§Ø±Ø§Ø­Øª', 'Ø§Ø¹ØµØ§Ø¨', 'Ø¨Ø¯'],
        'ØºÙ…Ú¯ÛŒÙ†': ['Ø§Ù†Ø¯ÙˆÙ‡', 'Ú¯Ø±ÛŒÙ‡', 'Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù†']
    }

    def __init__(self):
        self.sentiment_model = pipeline(
            "text-classification", 
            model="parsi-sentiment-analysis"
        )
        self.emotion_history = deque(maxlen=100)
        self.context_weights = {
            'Ø®ÙˆØ´Ø­Ø§Ù„': 0.7,
            'Ø¹ØµØ¨Ø§Ù†ÛŒ': 0.9,
            'ØºÙ…Ú¯ÛŒÙ†': 0.8
        }

    def _calculate_emotion_vector(self, text: str) -> Dict[str, float]:
        scores = {}
        tokens = text.split()
        for emotion, keywords in self.EMOTION_LEXICON.items():
            count = sum(1 for word in tokens if word in keywords)
            scores[emotion] = count / len(tokens) if tokens else 0
        return scores

    def analyze(self, text: str) -> Dict[str, Any]:
        sentiment = self.sentiment_model(text)[0]
        emotion_vector = self._calculate_emotion_vector(text)
        self.emotion_history.append(emotion_vector)
        return {
            'sentiment': sentiment['label'],
            'confidence': sentiment['score'],
            'emotion_vector': emotion_vector,
            'context_score': sum(
                self.context_weights[e] * v 
                for e, v in emotion_vector.items()
            )
        }

    def adjust_response(self, response: str, emotion: Dict) -> str:
        if emotion['sentiment'] == 'negative':
            return f"Ù…ØªØ§Ø³ÙÙ… Ú©Ù‡ Ø§ÛŒÙ†Ø·ÙˆØ± Ø§Ø­Ø³Ø§Ø³ Ù…ÛŒÚ©Ù†ÛŒ. {response}"
        if emotion['emotion_vector']['Ø¹ØµØ¨Ø§Ù†ÛŒ'] > 0.5:
            return f"Ù„Ø·ÙØ§ Ø¢Ø±Ø§Ù… Ø¨Ø§Ø´. {response}"
        return response

# ------------------ MemoryController ------------------
class MemoryController:
    def __init__(self):
        self.short_term = deque(maxlen=500)
        self.long_term = sqlite3.connect(':memory:')
        self._init_db()

    def _init_db(self):
        cursor = self.long_term.cursor()
        cursor.execute('''CREATE TABLE IF NOT EXISTS memories
                     (id TEXT PRIMARY KEY, 
                      data BLOB,
                      timestamp REAL,
                      weight REAL)''')
        self.long_term.commit()

    def store_interaction(self, input_data: str, output_data: str, metadata: Dict):
        mem_id = hashlib.sha3_512(input_data.encode()).hexdigest()
        data = {
            'input': input_data,
            'output': output_data,
            'metadata': metadata
        }
        cursor = self.long_term.cursor()
        cursor.execute('''INSERT INTO memories VALUES
                       (?, ?, ?, ?)''',
                       (mem_id, 
                        json.dumps(data).encode(),
                        time.time(),
                        metadata.get('cognitive_load', 0.5)))
        self.short_term.append(data)

    def recall(self, query: str, threshold=0.7) -> List[Dict]:
        cursor = self.long_term.cursor()
        cursor.execute('''SELECT data FROM memories 
                       WHERE weight > ? 
                       ORDER BY timestamp DESC''', (threshold,))
        return [json.loads(row[0]) for row in cursor.fetchall()]

# ------------------ SystemDiagnostics ------------------
class SystemDiagnostics:
    def __init__(self):
        self.metrics = {
            'latency': [],
            'memory_usage': [],
            'error_rates': defaultdict(int)
        }
        self.health_check_interval = 60
        self.last_check = time.time()

    def log_metric(self, name: str, value: float):
        if name == 'error':
            self.metrics['error_rates'][value] += 1
        else:
            self.metrics[name].append(value)

    def perform_health_check(self):
        now = time.time()
        if now - self.last_check > self.health_check_interval:
            report = {
                'avg_latency': np.mean(self.metrics['latency']),
                'max_memory': max(self.metrics['memory_usage'], default=0),
                'error_distribution': dict(self.metrics['error_rates'])
            }
            self._send_alert_if_needed(report)
            self.last_check = now

    def _send_alert_if_needed(self, report: Dict):
        if report['avg_latency'] > 5.0 or report['max_memory'] > 90.0:
            requests.post(
                "https://alert-system/api",
                json=report,
                timeout=1
            )

# ------------------ CircuitBreaker ------------------
class CircuitBreaker:
    def __init__(self, threshold=5, reset_timeout=60):
        self.failures = 0
        self.threshold = threshold
        self.reset_timeout = reset_timeout
        self.last_failure = 0
        self.state = 'CLOSED'

    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            if self.state == 'OPEN':
                if time.time() - self.last_failure > self.reset_timeout:
                    self.state = 'HALF-OPEN'
                else:
                    raise CircuitOpenException()
            try:
                result = func(*args, **kwargs)
                self._reset()
                return result
            except Exception as e:
                self._record_failure()
                raise
        return wrapper

    def _record_failure(self):
        self.failures += 1
        if self.failures >= self.threshold:
            self.state = 'OPEN'
            self.last_failure = time.time()

    def _reset(self):
        self.failures = 0
        self.state = 'CLOSED'

# ------------------ LRUCache ------------------
class LRUCache:
    def __init__(self, maxsize=128):
        self.cache = OrderedDict()
        self.maxsize = maxsize

    def __getitem__(self, key):
        self.cache.move_to_end(key)
        return self.cache[key]

    def __setitem__(self, key, value):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.maxsize:
            self.cache.popitem(last=False)

# ------------------ NeuroSymbolicReasoner ------------------
class NeuroSymbolicReasoner:
    def neural_process(self, text: str) -> Dict:
        inputs = self.tokenizer(text, return_tensors="pt")
        outputs = self.model(**inputs)
        return {
            "logits": outputs.logits,
            "embeddings": outputs.last_hidden_state.mean(dim=1)
        }
class MemoryVault:
    def __init__(self):
        self.vault = []
        self.security = QuantumSecurity()
        self.lock = Lock()
        
    def store(self, entry: Dict):
        with self.lock:
            if len(self.vault) >= config["system"]["max_history"]:
                self.vault.pop(0)
            self.vault.append(entry)
            self._backup()
    def _backup(self):
        encrypted = self.security.encrypt(json.dumps(self.vault))[0]
        Path("memory_vault.bin").write_bytes(encrypted)
class APIGateway:
    def handle_request(self, service_name: str, endpoint: str, **params):
        @self.circuit_breakers[service_name]
        def _call_api():
            try:
                start_time = time.time()
                result = self.services[service_name].call(endpoint, params)
                RESPONSE_TIME.observe(time.time() - start_time)
                return result
            except RequestException as e:
                ERROR_COUNTER.labels(type='network').inc()
                return self._handle_network_error(e)
            except APIError as e:
                ERROR_COUNTER.labels(type='api').inc()
                return self._handle_api_error(e)
                
        # Ù…Ù†Ø·Ù‚ Ú©Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        if cached := self._check_cache(service_name, endpoint, params):
            if self._validate_cached(cached):
                return cached
        return _call_api()
if __name__ == "__main__":
    print("ğŸ”¥ Ø³ÛŒØ³ØªÙ… TRIAD ÙØ¹Ø§Ù„ Ø´Ø¯! (Ø³Ø·Ø­ Ø§Ù…Ù†ÛŒØª Ú©ÙˆØ§Ù†ØªÙˆÙ…ÛŒ)")
    system = TriadCognitiveSystem()
    start_http_server(8000)
    try:
        while True:
            user_input = input("\nØ´Ù…Ø§: ")
            if user_input.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "terminate"]:
                break
            
            result = system.process_query(user_input)
            print(f"\nØ¯ÛŒØ¬ÛŒØª: {result['response']}")
            print(f"âœ‰ï¸ Ù‡Ø´ ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ: {result['integrity_check']}")
            
    finally:
        print("\nğŸ”’ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ SHA3-512 Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯!")